import logging
import math
import re

from core.llm_client import LLMClient

logger = logging.getLogger('app')


class AIDetectorService:
    CATEGORIES = [
        'ai_generated',
        'ai_generated_ai_refined',
        'human_written_ai_refined',
        'human_written',
    ]

    CATEGORY_LABELS = {
        'ai_generated': 'AI-Generated',
        'ai_generated_ai_refined': 'AI-Generated & AI-Refined',
        'human_written_ai_refined': 'Human-Written & AI-Refined',
        'human_written': 'Human-Written',
    }

    CATEGORY_COLORS = {
        'ai_generated': 'red',
        'ai_generated_ai_refined': 'orange',
        'human_written_ai_refined': 'yellow',
        'human_written': 'green',
    }

    CATEGORY_DESCRIPTIONS = {
        'ai_generated': 'This text appears to be entirely generated by AI with no human involvement.',
        'ai_generated_ai_refined': 'This text appears to be AI-generated and then further polished or refined by AI tools.',
        'human_written_ai_refined': 'This text appears to be originally written by a human but edited or polished using AI tools.',
        'human_written': 'This text appears to be entirely written by a human with no AI assistance.',
    }

    @staticmethod
    def _get_classification(score):
        """Legacy classification based on single score for sentence-level labels."""
        if score >= 80:
            return 'ai_generated'
        elif score >= 60:
            return 'ai_generated_ai_refined'
        elif score >= 30:
            return 'human_written_ai_refined'
        else:
            return 'human_written'

    @staticmethod
    def _get_label(score):
        """Legacy label based on single score for sentence-level labels."""
        if score >= 80:
            return 'AI-Generated'
        elif score >= 60:
            return 'AI-Generated & AI-Refined'
        elif score >= 30:
            return 'Human-Written & AI-Refined'
        else:
            return 'Human-Written'

    @staticmethod
    def _get_color(score):
        if score >= 80:
            return 'red'
        elif score >= 60:
            return 'orange'
        elif score >= 30:
            return 'yellow'
        else:
            return 'green'

    @staticmethod
    def _split_sentences(text):
        """Split text into sentences."""
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        return [s.strip() for s in sentences if s.strip()]

    @staticmethod
    def _compute_perplexity_heuristics(text):
        """
        Compute simple perplexity-based heuristics for AI detection.
        Returns a base score adjustment based on text characteristics.
        """
        sentences = AIDetectorService._split_sentences(text)
        if not sentences:
            return 0

        # Heuristic 1: Sentence length uniformity (AI tends to be uniform)
        lengths = [len(s.split()) for s in sentences]
        if len(lengths) > 1:
            avg_len = sum(lengths) / len(lengths)
            variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths)
            std_dev = math.sqrt(variance) if variance > 0 else 0
            # Low variance = more AI-like
            uniformity_score = max(0, 10 - std_dev) * 2
        else:
            uniformity_score = 5

        # Heuristic 2: Vocabulary diversity (AI tends to use more diverse vocab)
        words = text.lower().split()
        if words:
            unique_ratio = len(set(words)) / len(words)
            # Very high unique ratio can indicate AI
            vocab_score = unique_ratio * 15
        else:
            vocab_score = 0

        # Heuristic 3: Common AI transition phrases
        ai_phrases = [
            'furthermore', 'moreover', 'additionally', 'in conclusion',
            'it is important to note', 'it is worth noting', 'in essence',
            'delve', 'tapestry', 'multifaceted', 'nuanced', 'comprehensive',
            'robust', 'leverage', 'paradigm', 'holistic', 'synergy',
            'in today\'s world', 'in today\'s digital age', 'in this article',
            'as we navigate', 'it\'s important to remember',
        ]
        text_lower = text.lower()
        phrase_count = sum(1 for phrase in ai_phrases if phrase in text_lower)
        phrase_score = min(phrase_count * 8, 30)

        return min(uniformity_score + vocab_score + phrase_score, 50)

    @staticmethod
    def _score_to_confidences(score):
        """Derive 4-category confidence distribution from a blended score (0-100)."""
        if score >= 80:
            return {'ai_generated': 70, 'ai_generated_ai_refined': 20, 'human_written_ai_refined': 7, 'human_written': 3}
        elif score >= 60:
            return {'ai_generated': 20, 'ai_generated_ai_refined': 55, 'human_written_ai_refined': 18, 'human_written': 7}
        elif score >= 30:
            return {'ai_generated': 5, 'ai_generated_ai_refined': 15, 'human_written_ai_refined': 55, 'human_written': 25}
        else:
            return {'ai_generated': 3, 'ai_generated_ai_refined': 7, 'human_written_ai_refined': 20, 'human_written': 70}

    @staticmethod
    def detect(text, use_premium=False):
        """
        Analyze text for AI-generated content using DeBERTa classifier + heuristics.
        Returns 4-category classification with confidence for each category.

        Args:
            text: Text to analyze
            use_premium: Whether to use premium tier (unused, kept for API compat)

        Returns:
            tuple: (result_dict, error_string)
            result_dict contains: overall_score, classification, category_confidences, sentences
        """
        sentences = AIDetectorService._split_sentences(text)
        if not sentences:
            return None, 'No sentences found in the provided text.'

        # Get heuristic score
        heuristic_score = AIDetectorService._compute_perplexity_heuristics(text)

        # Call DeBERTa model on GPU server
        model_result, error = LLMClient.detect_ai_text(text)

        if error:
            # Fallback to heuristics-only if model unavailable
            logger.warning(f'AI detect model unavailable, falling back to heuristics: {error}')
            blended_score = max(0, min(100, round(heuristic_score)))
            category_confidences = AIDetectorService._score_to_confidences(blended_score)
            classification = max(category_confidences, key=category_confidences.get)

            analyzed_sentences = []
            for s in sentences:
                analyzed_sentences.append({
                    'text': s,
                    'score': blended_score,
                    'label': AIDetectorService._get_label(blended_score),
                    'color': AIDetectorService._get_color(blended_score),
                })

            return {
                'overall_score': blended_score,
                'classification': classification,
                'classification_label': AIDetectorService.CATEGORY_LABELS.get(classification, 'Unknown'),
                'classification_description': AIDetectorService.CATEGORY_DESCRIPTIONS.get(classification, ''),
                'category_confidences': category_confidences,
                'sentences': analyzed_sentences,
            }, None

        # Blend: model score (85%) + heuristic score (15%)
        model_score = model_result.get('score', 50)
        blended_score = round((model_score * 0.85) + (heuristic_score * 0.15))
        blended_score = max(0, min(100, blended_score))

        # Derive category confidences from blended score
        category_confidences = AIDetectorService._score_to_confidences(blended_score)
        classification = max(category_confidences, key=category_confidences.get)
        classification_label = AIDetectorService.CATEGORY_LABELS.get(classification, 'Unknown')

        # Build sentence-level analysis using heuristic per-sentence scoring
        analyzed_sentences = []
        for s in sentences:
            # Use heuristic to approximate per-sentence score,
            # anchored to the overall model score
            sentence_heuristic = AIDetectorService._compute_perplexity_heuristics(s)
            sentence_score = round((model_score * 0.85) + (sentence_heuristic * 0.15))
            sentence_score = max(0, min(100, sentence_score))
            analyzed_sentences.append({
                'text': s,
                'score': sentence_score,
                'label': AIDetectorService._get_label(sentence_score),
                'color': AIDetectorService._get_color(sentence_score),
            })

        return {
            'overall_score': blended_score,
            'classification': classification,
            'classification_label': classification_label,
            'classification_description': AIDetectorService.CATEGORY_DESCRIPTIONS.get(classification, ''),
            'category_confidences': category_confidences,
            'sentences': analyzed_sentences,
        }, None

    @staticmethod
    def detect_text_from_file(file_content, filename, use_premium=False):
        """
        Run AI detection on extracted text from a file.
        Returns (result_dict, error_string) same as detect().
        """
        text = file_content.strip()
        if not text:
            return None, f'No text content found in {filename}.'

        word_count = len(text.split())
        if word_count < 80:
            return None, f'{filename}: Text too short ({word_count} words, minimum 80).'

        result, error = AIDetectorService.detect(text, use_premium=use_premium)
        if error:
            return None, f'{filename}: {error}'

        result['filename'] = filename
        result['word_count'] = word_count
        return result, None
