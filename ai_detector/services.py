import json
import logging
import re
import math

from core.llm_client import LLMClient

logger = logging.getLogger('app')


class AIDetectorService:
    CATEGORIES = [
        'ai_generated',
        'ai_generated_ai_refined',
        'human_written_ai_refined',
        'human_written',
    ]

    CATEGORY_LABELS = {
        'ai_generated': 'AI-Generated',
        'ai_generated_ai_refined': 'AI-Generated & AI-Refined',
        'human_written_ai_refined': 'Human-Written & AI-Refined',
        'human_written': 'Human-Written',
    }

    CATEGORY_COLORS = {
        'ai_generated': 'red',
        'ai_generated_ai_refined': 'orange',
        'human_written_ai_refined': 'yellow',
        'human_written': 'green',
    }

    CATEGORY_DESCRIPTIONS = {
        'ai_generated': 'This text appears to be entirely generated by AI with no human involvement.',
        'ai_generated_ai_refined': 'This text appears to be AI-generated and then further polished or refined by AI tools.',
        'human_written_ai_refined': 'This text appears to be originally written by a human but edited or polished using AI tools.',
        'human_written': 'This text appears to be entirely written by a human with no AI assistance.',
    }

    @staticmethod
    def _get_classification(score):
        """Legacy classification based on single score for sentence-level labels."""
        if score >= 80:
            return 'ai_generated'
        elif score >= 60:
            return 'ai_generated_ai_refined'
        elif score >= 30:
            return 'human_written_ai_refined'
        else:
            return 'human_written'

    @staticmethod
    def _get_label(score):
        """Legacy label based on single score for sentence-level labels."""
        if score >= 80:
            return 'AI-Generated'
        elif score >= 60:
            return 'AI-Generated & AI-Refined'
        elif score >= 30:
            return 'Human-Written & AI-Refined'
        else:
            return 'Human-Written'

    @staticmethod
    def _get_color(score):
        if score >= 80:
            return 'red'
        elif score >= 60:
            return 'orange'
        elif score >= 30:
            return 'yellow'
        else:
            return 'green'

    @staticmethod
    def _split_sentences(text):
        """Split text into sentences."""
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        return [s.strip() for s in sentences if s.strip()]

    @staticmethod
    def _compute_perplexity_heuristics(text):
        """
        Compute simple perplexity-based heuristics for AI detection.
        Returns a base score adjustment based on text characteristics.
        """
        sentences = AIDetectorService._split_sentences(text)
        if not sentences:
            return 0

        # Heuristic 1: Sentence length uniformity (AI tends to be uniform)
        lengths = [len(s.split()) for s in sentences]
        if len(lengths) > 1:
            avg_len = sum(lengths) / len(lengths)
            variance = sum((l - avg_len) ** 2 for l in lengths) / len(lengths)
            std_dev = math.sqrt(variance) if variance > 0 else 0
            # Low variance = more AI-like
            uniformity_score = max(0, 10 - std_dev) * 2
        else:
            uniformity_score = 5

        # Heuristic 2: Vocabulary diversity (AI tends to use more diverse vocab)
        words = text.lower().split()
        if words:
            unique_ratio = len(set(words)) / len(words)
            # Very high unique ratio can indicate AI
            vocab_score = unique_ratio * 15
        else:
            vocab_score = 0

        # Heuristic 3: Common AI transition phrases
        ai_phrases = [
            'furthermore', 'moreover', 'additionally', 'in conclusion',
            'it is important to note', 'it is worth noting', 'in essence',
            'delve', 'tapestry', 'multifaceted', 'nuanced', 'comprehensive',
            'robust', 'leverage', 'paradigm', 'holistic', 'synergy',
            'in today\'s world', 'in today\'s digital age', 'in this article',
            'as we navigate', 'it\'s important to remember',
        ]
        text_lower = text.lower()
        phrase_count = sum(1 for phrase in ai_phrases if phrase in text_lower)
        phrase_score = min(phrase_count * 8, 30)

        return min(uniformity_score + vocab_score + phrase_score, 50)

    @staticmethod
    def detect(text, use_premium=False):
        """
        Analyze text for AI-generated content using LLM + perplexity heuristics.
        Returns 4-category classification with confidence for each category.

        Args:
            text: Text to analyze
            use_premium: Whether to use premium tier for LLM calls

        Returns:
            tuple: (result_dict, error_string)
            result_dict contains: overall_score, classification, category_confidences, sentences
        """
        sentences = AIDetectorService._split_sentences(text)
        if not sentences:
            return None, 'No sentences found in the provided text.'

        # Get heuristic base score
        heuristic_score = AIDetectorService._compute_perplexity_heuristics(text)

        try:
            prompt = f"""Analyze the following text and determine if it was written by AI or a human.

Classify the text into one of these 4 categories:
1. "ai_generated" - 100% AI written, no human involvement
2. "ai_generated_ai_refined" - AI written, then further polished/edited by AI tools
3. "human_written_ai_refined" - Human written, then edited/polished using AI tools
4. "human_written" - 100% human written, no AI assistance

For each sentence, provide a probability score from 0 to 100 indicating how likely it is AI-generated (100 = definitely AI, 0 = definitely human).

Consider these factors:
- Sentence structure patterns (AI tends to be more uniform)
- Vocabulary choices (AI uses certain words more frequently like "delve", "tapestry", "multifaceted")
- Transition phrases (AI relies heavily on "Furthermore", "Moreover", "Additionally")
- Natural imperfections (humans make more varied sentence constructions)
- Personal voice and style (humans tend to have more unique voice)
- Specificity vs generality (AI tends to be more general)
- Signs of AI refinement (overly polished human text, inconsistent style within text)

Return your analysis as a JSON object with this exact format:
{{
    "classification": "one of: ai_generated, ai_generated_ai_refined, human_written_ai_refined, human_written",
    "category_confidences": {{
        "ai_generated": 15,
        "ai_generated_ai_refined": 25,
        "human_written_ai_refined": 40,
        "human_written": 20
    }},
    "sentences": [
        {{"text": "sentence text here", "score": 75}},
        ...
    ],
    "overall_score": 65
}}

The category_confidences must sum to 100 and represent the probability that the text belongs to each category.

Text to analyze:
{text}

Return ONLY the JSON object, no other text."""

            response_text, error = LLMClient.generate(
                system_prompt=None,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=4096,
                use_premium=use_premium
            )

            if error:
                logger.error(f'LLM error in AI detector: {error}')
                return None, 'AI detection service is temporarily unavailable. Please try again.'

            response_text = response_text.strip()

            # Extract JSON from response
            json_match = re.search(r'\{[\s\S]*\}', response_text)
            if not json_match:
                return None, 'Failed to parse AI detection results.'

            result = json.loads(json_match.group())

            # Blend Claude's score with heuristic score
            claude_score = result.get('overall_score', 50)
            blended_score = round((claude_score * 0.7) + (heuristic_score * 0.3))
            blended_score = max(0, min(100, blended_score))

            # Extract category confidences from LLM
            raw_confidences = result.get('category_confidences', {})
            category_confidences = {}
            for cat in AIDetectorService.CATEGORIES:
                category_confidences[cat] = max(0, min(100, int(raw_confidences.get(cat, 0))))

            # Normalize confidences to sum to 100
            total = sum(category_confidences.values())
            if total > 0 and total != 100:
                for cat in category_confidences:
                    category_confidences[cat] = round(category_confidences[cat] * 100 / total)
                # Fix rounding errors
                diff = 100 - sum(category_confidences.values())
                if diff != 0:
                    # Add difference to the highest confidence category
                    max_cat = max(category_confidences, key=category_confidences.get)
                    category_confidences[max_cat] += diff
            elif total == 0:
                # Fallback: derive from blended score
                if blended_score >= 80:
                    category_confidences = {'ai_generated': 70, 'ai_generated_ai_refined': 20, 'human_written_ai_refined': 7, 'human_written': 3}
                elif blended_score >= 60:
                    category_confidences = {'ai_generated': 20, 'ai_generated_ai_refined': 55, 'human_written_ai_refined': 18, 'human_written': 7}
                elif blended_score >= 30:
                    category_confidences = {'ai_generated': 5, 'ai_generated_ai_refined': 15, 'human_written_ai_refined': 55, 'human_written': 25}
                else:
                    category_confidences = {'ai_generated': 3, 'ai_generated_ai_refined': 7, 'human_written_ai_refined': 20, 'human_written': 70}

            # Determine final classification from LLM or from highest confidence
            llm_classification = result.get('classification', '')
            if llm_classification in AIDetectorService.CATEGORIES:
                classification = llm_classification
            else:
                classification = max(category_confidences, key=category_confidences.get)

            classification_label = AIDetectorService.CATEGORY_LABELS.get(classification, 'Unknown')

            # Process sentences
            analyzed_sentences = []
            for item in result.get('sentences', []):
                score = max(0, min(100, item.get('score', 50)))
                analyzed_sentences.append({
                    'text': item.get('text', ''),
                    'score': score,
                    'label': AIDetectorService._get_label(score),
                    'color': AIDetectorService._get_color(score),
                })

            # If LLM returned fewer sentences than we have, fill in
            if len(analyzed_sentences) < len(sentences):
                for i in range(len(analyzed_sentences), len(sentences)):
                    analyzed_sentences.append({
                        'text': sentences[i],
                        'score': blended_score,
                        'label': AIDetectorService._get_label(blended_score),
                        'color': AIDetectorService._get_color(blended_score),
                    })

            return {
                'overall_score': blended_score,
                'classification': classification,
                'classification_label': classification_label,
                'classification_description': AIDetectorService.CATEGORY_DESCRIPTIONS.get(classification, ''),
                'category_confidences': category_confidences,
                'sentences': analyzed_sentences,
            }, None

        except json.JSONDecodeError as e:
            logger.error(f'JSON decode error in AI detector: {str(e)}')
            return None, 'Failed to parse AI detection results.'
        except Exception as e:
            logger.error(f'Unexpected error in AI detector: {str(e)}')
            return None, 'An unexpected error occurred. Please try again.'

    @staticmethod
    def detect_text_from_file(file_content, filename, use_premium=False):
        """
        Run AI detection on extracted text from a file.
        Returns (result_dict, error_string) same as detect().
        """
        text = file_content.strip()
        if not text:
            return None, f'No text content found in {filename}.'

        word_count = len(text.split())
        if word_count < 80:
            return None, f'{filename}: Text too short ({word_count} words, minimum 80).'

        result, error = AIDetectorService.detect(text, use_premium=use_premium)
        if error:
            return None, f'{filename}: {error}'

        result['filename'] = filename
        result['word_count'] = word_count
        return result, None
